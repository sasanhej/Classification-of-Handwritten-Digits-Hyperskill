type: edu
files:
- name: analysis.py
  visible: true
  text: |
    import tensorflow as tf
    import pandas as pd
    import numpy as np
    from sklearn.model_selection import train_test_split

    (X_train, y_train) = tf.keras.datasets.mnist.load_data(path="mnist.npz")[0]
    X_train = X_train.reshape(-1, 784)
    '''
    #Stage 1/5
    print(f"Classes: {np.unique(y_train)}")
    print(f"Features' shape: {X_train.shape}")
    print(f"Target's shape: {y_train.shape}")
    print(f"min: {X_train.min()}, max: {X_train.max()}")
    '''

    #Stage 2/5
    X_train, X_test, y_train, y_test = train_test_split(X_train[:6000], y_train[:6000], test_size=0.3, random_state=40)
    print(f"x_train shape: {X_train.shape}")
    print(f"x_test shape: {X_test.shape}")
    print(f"y_train shape: {y_train.shape}")
    print(f"y_test shape: {y_test.shape}")
    print('Proportion of samples per class in train set:')
    print(pd.Series(y_train).value_counts(normalize=True))
  learner_created: false
- name: test/__init__.py
  visible: false
  learner_created: false
- name: test/tests.py
  visible: false
  text: |
    from hstest.stage_test import StageTest
    from hstest.test_case import TestCase
    from hstest.check_result import CheckResult
    from sklearn.exceptions import ConvergenceWarning
    import warnings

    import re

    # turn off logistic regression convergence warning
    warnings.filterwarnings("ignore", category=ConvergenceWarning)


    # function to provide better feedback
    def get_model_name(line_reply):
        idx = line_reply.replace(" ", "").lower().index('model:') + len('model:')
        model_name_reply = line_reply.replace(" ", "")[idx:]
        return model_name_reply


    def get_lines_with_key_words(lines, keywords):
        lines_with_keywords = []
        for line in lines:
            if set(line.lower().split()) & set(keywords):
                lines_with_keywords.append(line)
        return lines_with_keywords


    class CCATest(StageTest):

        def generate(self):
            return [TestCase(time_limit=60000)]

        def check(self, reply, attach):
            lines = reply.split('\n')
            if "" in lines:
                lines = list(filter(lambda a: a != "", lines))

            relevant_lines = get_lines_with_key_words(lines, keywords=['model:', 'accuracy:', 'question:'])

            # general
            if len(relevant_lines) != 9:
                return CheckResult.wrong(
                    feedback=f"Expected 9 lines with \"Model:\"/\"Accuracy:\"/\"Answer to the question:\", found {len(relevant_lines)}\n"
                             f"Note that the order of the models in the output is important (see the Example section)")

            # models and accuracies print
            # 1st model
            model_name_answer = 'KNeighborsClassifier'
            if model_name_answer not in relevant_lines[0]:
                model_name_reply = get_model_name(relevant_lines[0])
                return CheckResult.wrong(feedback=f"Incorrect name of the 1st model\n"
                                                  f"Expected {model_name_answer}, found {model_name_reply}")

            accuracy_reply = re.findall(r'\d*\.\d+|\d+', relevant_lines[1])
            if len(accuracy_reply) != 1:
                return CheckResult.wrong(feedback=f'It should be one number in the "Accuracy:" section')
            # 1% error rate is allowed, right accuracy = 0.935
            if not 0.99 * 0.935 < float(accuracy_reply[0]) < 1.01 * 0.935:
                return CheckResult.wrong(feedback=f"Wrong accuracy for the 1st model")

            # 2nd model
            model_name_answer = 'DecisionTreeClassifier'
            if model_name_answer not in relevant_lines[2]:
                model_name_reply = get_model_name(relevant_lines[2])
                return CheckResult.wrong(feedback=f"Incorrect name of the 1st model\n"
                                                  f"Expected {model_name_answer}, found {model_name_reply}")

            accuracy_reply = re.findall(r'\d*\.\d+|\d+', relevant_lines[3])
            if len(accuracy_reply) != 1:
                return CheckResult.wrong(feedback=f'It should be one number in the "Accuracy:" section')
            # 2% error rate is allowed, right accuracy = 0.761
            if not 0.98 * 0.761 < float(accuracy_reply[0]) < 1.02 * 0.761:
                return CheckResult.wrong(feedback=f"Wrong accuracy for the 2nd model")

            # 3rd model
            model_name_answer = 'LogisticRegression'
            if model_name_answer not in relevant_lines[4]:
                model_name_reply = get_model_name(relevant_lines[4])
                return CheckResult.wrong(feedback=f"Incorrect name of the 1st model\n"
                                                  f"Expected {model_name_answer}, found {model_name_reply}")

            accuracy_reply = re.findall(r'\d*\.\d+|\d+', relevant_lines[5])
            if len(accuracy_reply) != 1:
                return CheckResult.wrong(feedback=f'It should be one number in the "Accuracy:" section')
            # 2% error rate is allowed, right accuracy = 0.874
            if not float(accuracy_reply[0]) > 0.8:
                return CheckResult.wrong(feedback=f"Wrong accuracy for the 3rd model")

            # 4th model
            model_name_answer = 'RandomForestClassifier'
            if model_name_answer not in relevant_lines[6]:
                model_name_reply = get_model_name(relevant_lines[6])
                return CheckResult.wrong(feedback=f"{model_name_reply} is incorrect name of the 4th model\n"
                                                  f"Expected {model_name_answer}, found {model_name_reply}")

            accuracy_reply = re.findall(r'\d*\.\d+|\d+', relevant_lines[7])
            if len(accuracy_reply) != 1:
                return CheckResult.wrong(feedback=f'It should be one number in the "Accuracy:" section')
            # 1% error rate is allowed, right accuracy = 0.939
            if not 0.99 * 0.939 < float(accuracy_reply[0]) < 1.01 * 0.939:
                return CheckResult.wrong(feedback=f"Wrong accuracy for the 4th model")

            # answer to the question
            answer_reply = relevant_lines[8].replace(" ", "").split('question:')[1]
            if 'RandomForestClassifier' not in answer_reply:
                return CheckResult.wrong(feedback=f'Wrong name of the model in "The answer to the question:" section')
            best_accuracy_reply = re.findall(r'\d*\.\d+|\d+', relevant_lines[8].replace(" ", "").split('question:')[1])
            if len(best_accuracy_reply) != 1:
                return CheckResult.wrong(
                    feedback=f'It should be one number, which represents acccuracy, in "The answer to the question:" section')
            if not 0.99 * 0.939 < float(best_accuracy_reply[0]) < 1.01 * 0.939:
                return CheckResult.wrong(feedback=f'Wrong accuracy in "The answer to the question:" section')

            return CheckResult.correct()


    if __name__ == '__main__':
        CCATest().run_tests()
  learner_created: false
- name: tests.py
  visible: false
  text: |
    from test.tests import CCATest

    if __name__ == '__main__':
        CCATest().run_tests()
  learner_created: false
feedback_link: https://hyperskill.org/learn/step/15235#comment
status: Failed
record: 2
